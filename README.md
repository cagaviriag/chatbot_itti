# ğŸ§  - Chatbot GenAI para Fintech + MÃ³dulo de EvaluaciÃ³n

**Chatbot** es una soluciÃ³n completa basada en IA generativa diseÃ±ada para asistir a usuarios de una **fintech** en temas relacionados con **tarjetas de dÃ©bito**, **tarjetas de crÃ©dito** y **prÃ©stamos personales**. El proyecto incluye:

- Una interfaz conversacional (frontend con Streamlit)
- Una interfaz para la evaluaciÃ³n automÃ¡tica de la calidad de las respuestas (Ragas, ground-truth y LLM)
- Un motor de prompts estructurado
- Almacenamiento de conversaciones y resultados de las evaluaciones de los modelos


---

## ğŸ§­ Estructura del proyecto

```
ITTI/
â”œâ”€â”€ app.py                        # Interfaz principal con Streamlit para flujo conversacional y evaluaciÃ³n automÃ¡tica
â”œâ”€â”€ coree/
â”‚   â”œâ”€â”€ prompt_manager.py         # Carga y rellenado de prompts .md
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ model_client.py           # Cliente para invocar modelos LLM
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ system_prompt.md
â”‚   â”œâ”€â”€ producto_contex_prompt.md
â”‚   â”œâ”€â”€ evalua_respuesta.md
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ conversations/            # Conversaciones guardadas (.json)
â”‚   â””â”€â”€ reference_dataset.xlsx
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ evaluator.py              # Orquestador para evaluaciÃ³n se llama desde app.py
â”‚   â”œâ”€â”€ ground_truth_eval.py
â”‚   â”œâ”€â”€ llm_eval.py
â”‚   â”œâ”€â”€ ragas_eval.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â”œâ”€â”€ results/
â”‚   â””â”€â”€ excel_to_reference_json.py
â”œâ”€â”€ entregables/                # Entregables solicitado en la prueba, pdf, word, etc
â””â”€â”€ requirements.txt
```

---

## ğŸš€ Â¿QuÃ© hace este proyecto?

| MÃ³dulo | Funcionalidad |
|--------|----------------|
| ğŸ’¬ `app.py` | Frontend conversacional con Streamlit y modulo de evaluacion de las interaciones |
| ğŸ“¦ `prompt_manager.py` | Manejo modular de prompts `.md` con placeholders |
| ğŸ§  `model_client.py` | Interfaz para enviar prompts al LLM seleccionado |
| ğŸ—ƒï¸ `data/conversations/` | Guarda cada conversaciÃ³n como un `.json` estructurado |
| ğŸ“Š `evaluation/` | Contiene toda la lÃ³gica de evaluaciÃ³n del chatbot |
| ğŸ“‘ `reference_dataset.xlsx` | Base editable (ground-truth) de preguntas y respuestas |
| ğŸ” `excel_to_reference_json.py` | Convierte el Excel en un JSON estructurado para evaluar |
| âœ… `evaluator.py` | Orquesta las evaluaciones (ground-truth, LLM o Ragas) |

---

## ğŸ› ï¸ Requisitos

Instala las dependencias necesarias:

```bash
pip install -r requirements.txt
```

---

## ğŸ” Variables de Entorno

Para garantizar la **reproducibilidad**, debes crear un archivo `.env` en la raÃ­z del proyecto con las siguientes variables:

```env
AWS_REGION_NAME=your-region
AWS_ACCESS_KEY_ID=your-access-key
AWS_SECRET_ACCESS_KEY=your-secret-key
AWS_SESSION_TOKEN=your-session-token
OPENAI_API_KEY=your-openai-key


---

## ğŸ’» CÃ³mo ejecutar la app conversacional o evaluaciÃ³n de modelos

```bash
streamlit run app.py
```

---

## ğŸ“¥ Â¿CÃ³mo se guardan las conversaciones?

Cada interacciÃ³n se guarda automÃ¡ticamente al hacer clic en el botÃ³n `Finalizar chat` dentro de `app.py`. Esto genera un archivo `.json` dentro de:

```
data/conversations/
```

Con estructura estÃ¡ndar:

```json
{
  "timestamp": "...",
  "session_id": "...",
  "modelo": "...",
  "turns": [
    {"role": "user", "content": "..."},
    {"role": "assistant", "content": "..."}
  ]
}
```

---

## âœ… EvaluaciÃ³n de las conversaciones

### 1. Preparar el dataset de ground-truth

Edita `data/reference_dataset.xlsx` con columnas:

- `pregunta`
- `respuesta_esperada`

Luego, conviÃ©rtelo a JSON:

```bash
python evaluation/excel_to_reference_json.py
```

Esto generarÃ¡: `evaluation/reference_dataset.json`

---

### 2. Ejecutar evaluaciÃ³n, la evaluacion se puede realizar por Ragas, ground-truth, evaluacion de otro llm, o las 3 juntas.

```bash
# EvaluaciÃ³n comparando con respuestas esperadas
python app.py  ## en el front desde la pestaÃ±a de evaluacion de modelos

```

Los resultados se guardan en:

```
evaluation/results/
```

---

## ğŸ“Š Resultados generados

Cada evaluaciÃ³n produce un archivo `.json` con mÃ©tricas como:

- `tipo_eval`,`session_id`,`respuesta`, `esperada`, `pregunta`
- `match_score` (ground-truth)
- `evaluacion_llm` (opiniÃ³n estructurada de otro modelo)
- `faithfulness` y  `answer_relevancy` (Ragas)


Estos archivos pueden usarse para visualizaciones, dashboards o retroalimentaciÃ³n al equipo de producto.

---

## ğŸ§  Estilo del asistente (prompting)

El agente sigue principios de:
- Claridad, precisiÃ³n y empatÃ­a
- Razonamiento tipo CoT ("Primeroâ€¦ Luegoâ€¦ Finalmenteâ€¦")
- Few-shot prompting con ejemplos conversacionales
- No responde fuera del dominio financiero
- Usa placeholders como `{{nombre_fintech}}` para inyecciÃ³n dinÃ¡mica y `{{contexto_productos}}` para agregar la informacion de los productos.

---

## ğŸ“Œ Notas adicionales

- Puedes personalizar los prompts en la carpeta `/prompts`
- Puedes extender el evaluador para incluir anotaciÃ³n humana, RAG

---

## ğŸ§‘â€ğŸ’» Autor / Mantenimiento

Este repositorio fue diseÃ±ado para servir como base sÃ³lida para asistentes conversacionales financieros evaluables en entornos reales por Cristian Gaviria